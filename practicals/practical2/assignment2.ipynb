{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 2 How to learn with neural networks?\n",
    "## <font color='green'>[+100 points in total, +30 optional points]</font>\n",
    "\n",
    "This assignment contains several files that need to be completed/implemented. This Jypyter notebook serves only as a \"glue\", to connect everything together and let you run the code more easily. If you want to run a particular file, you just need to type in a standard code cell the following\n",
    "```lua\n",
    "dofile 'mynewfile.lua'\n",
    "```\n",
    "and it will run your code as if you would type that in the terminal.\n",
    "\n",
    "The rest of this notebook will describe the assignment, present to you the questions that you need to complete and guide you through through the answer. You should not use this notebook to actually program the code for the assignment, other than calling \"main\" files to get, print and plot the results. You should provide the textual as well as the numerical answers inline, after each question by running the right scripts that you should have implemented. Once you complete the assignment and answer the questions inline, you can download the report in pdf (File->Download as->PDF) and send it to us, together with the code.\n",
    "\n",
    "To share your code, results and pdf with us, please make a Bitbucket repository and invite us to your project. Then, we will clone your project and correct your assigmment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "Implement your code and answer all the questions. Make your own bitbucket repository (bitbucket allows for private repositories). Commit your answers to the repository and invite us to access your solutions. Please send your answers by February 18, at 23:59.\n",
    "In the course there is a 7 late day policy. This means that you are allowed 7 days in total for delivering the assignments, you can use them as you please. Beyone that each extra late day will have a 10% penalty on your final score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "By the end of this practical you should:\n",
    "<ul>\n",
    "<li>be able to derive the correct backpropagation equations for an arbitrary neural network</li>\n",
    "<li>be able to implement a standard multi-layer perceptron (MLP) with various architectures.\n",
    "MLP is the most basic neural network and can be used for a variety of tasks, such classification and regression.</li>\n",
    "<li>be comfortable with picking the right hyperparameters for your neural network given a task.</li>\n",
    "<li>be able to implement your own basic module (or layer) in Torch.</li>\n",
    "<li>be able to implement your own optimization function with which you can train your neural network.</li>\n",
    "<li>Optionally, you should be able to write your own backpropagation algorithm by implementing the basic equations. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 Implement a multi-layer perceptron (MLP)\n",
    "\n",
    "First, we will implement our very first multilayer perceptron, commonly referred to as MLP.\n",
    "An MLP is a feedforward neural network.\n",
    "This means that is composed of multiple layers, or modules as they referred to in the Torch jargon.\n",
    "The possible design choices for an MLP, outside the learning hyperparameters, are the following:\n",
    "\n",
    "<ul>\n",
    "<li>number of modules</li>\n",
    "<li>number of units in each module</li>\n",
    "<li>type of operation applied inside each module</li>\n",
    "<li>type of loss function</li>\n",
    "</ul>\n",
    "\n",
    "Your starter code is composed of the following files: <tt>load_data_mnist.lua</tt>, <tt>preprocess_data_mnist.lua</tt>, <tt>define_model.lua</tt>, <tt>define_training.lua</tt>, <tt>define_testing.lua</tt> and <tt>assignment2.lua</tt>.\n",
    "Inside these functions there are <tt>TODO</tt> items that need to be completed before running any full scale experiment.\n",
    "\n",
    "The <tt>assignment2.lua</tt> file is the wrapper that connects all the other files. \n",
    "It's split into 3 parts where the experiment parameters are configured and 5 parts, where the different functions are called.\n",
    "Once all files are fully implemented, you can run an experiment with\n",
    "```lua\n",
    "dofile 'assignment2.lua'\n",
    "```\n",
    "\n",
    "Next, we guide you through the implementation.\n",
    "\n",
    "## Section 2.1 Define the experiment setup\n",
    "\n",
    "Before you start with anything, you need to define the setup of your experiments.\n",
    "Here you can <tt>require</tt> all your Torch packages or define the torch parameters for running the epxeriment.\n",
    "Also, you can define the model and optimization hyper-parameters.\n",
    "\n",
    "Open your <tt>assignment2.lua</tt> file and go to section 2.1.\n",
    "Fill in the missing values in the TODO parts.\n",
    "\n",
    "### <font color='green'>Question A.1 [+5 points]</font>\n",
    "\n",
    "<font color='green'>In the experiment options you need to define\n",
    "<ul>\n",
    "<li>the loss functions in <tt>opt['loss']</tt>.\n",
    "You can check the different options here https://github.com/torch/nn/blob/master/doc/criterion.md. \n",
    "You can start with the <tt>Margin Criterion</tt>, which is essentially the SVM loss.</li>\n",
    "<li>the optimization method that you are going to use in <tt>opt['optimization']</tt>.\n",
    "You can check the different options here http://optim.readthedocs.org/en/latest/.\n",
    "You can start with <tt>SGD</tt>.</li>\n",
    "</ul></font>\n",
    "\n",
    "<font color='green'>\n",
    "In the model options you need to define\n",
    "<ul>\n",
    "<li> The number of hidden units.\n",
    "You can start with 100 units.</li>\n",
    "<li> The (mini-) batch size.\n",
    "You can start with 1, meaning after each new training sample you make a parameter update with gradient descend.</li>\n",
    "<li> The learning rate.\n",
    "You can start with <tt>1e-3</tt>, which is a reasonable choice, see lecture notes.</li>\n",
    "<li> The weight decay is equivalent to adding a regularization on your model parameters.\n",
    "You can start with 0.0, in which case you have no regularization.</li>\n",
    "<li> The momentum for computing more robust gradient updatse.\n",
    "You can start with the vanilla version and set the momentum to 0.0.</li>\n",
    "</ul>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 2.2 Define the dataset\n",
    "\n",
    "Next, you need to define the dataset.\n",
    "In this experiment we are going to use the MNIST dataset, which is very popular in the neural network and deep learning community.\n",
    "It is not a very difficult dataset and the best state-of-the-art reaches accuracies up to 99.7-8%.\n",
    "Still, it's a good dataset for studying the essentials of neural networks.\n",
    "\n",
    "You will first need a wrapper for loading the data that you are going to process.\n",
    "We have already prepared a script for loading the MNIST dataset, which is invoked with the command\n",
    "```lua\n",
    "dofile 'load_data_mnist.lua'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3 Perform data preprocessing and normalization\n",
    "### <font color='green'> [Questions A.2-A.4: +10 points] </font>\n",
    "\n",
    "As discussed in the lectures, neural networks and deep learning prefer the input variables to contain as raw data as possible.\n",
    "Still, this does not mean that any input will do and in the vast majority of cases the data need to be preprocessed and normalized.\n",
    "\n",
    "The following 2 questions will outline the importance of data normalization on a toy, 1-d example.\n",
    "These questions are not directly related to the rest of the assigment though.\n",
    "\n",
    "### <font color='green'>Question A.2 Simple data normalization</font>\n",
    "\n",
    "<font color='green'>\n",
    "Open the dataset of points from \"points.txt\".\n",
    "When plotted with the x-axis range x=[0, 20] and the y-axis range [-15, 0], you will get the following image.\n",
    "\n",
    "<img src=\"images/toydataset.png\">\n",
    "\n",
    "Implement the gaussian normalization so that the two variables follow an $N(0, 1)$ distribution.\n",
    "Plot the distributions of features before and after and visually verify that the features are indeed normalized.</font>\n",
    "\n",
    "### <font color='green'>Question A.4</font>\n",
    "\n",
    "<font color='green'>\n",
    "Now that you have implemented the normalization for this simple dataset, do the same for the input features loaded from <tt>load_data_mnist.lua</tt>.\n",
    "Different from before, we have multiple pixels and therefore multiple dimensions per sample image.\n",
    "\n",
    "Normalize the input image pixels per channel.\n",
    "Namely, take the all pixel values per channel (red, green, blue) and compute the mean and standard deviation for each channel (3 means, 3 standard deviations).\n",
    "Print the mean and standard deviation values after normalization for both training and test set.\n",
    "Do you observe a difference between the normalized training and test features and why?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2 Define training, optimization, etc.\n",
    "### <font color='green'> [Questions A.5-A.6: +10 points] </font>\n",
    "\n",
    "Before working on our model and trying to find the best possible architecture and hyperparameters, we need to prepare the routines for training, namely learning the optimal model parameters to obtain the best possible accuracy for our model.\n",
    "\n",
    "### <font color='green'>Question A.5</font>\n",
    "\n",
    "<font color='green'>\n",
    "Open the <tt>define_training.lua</tt> file and go to <tt>TODO1</tt>.\n",
    "Write the code to split the data into minibatches of the predefined size.\n",
    "For this you can add the minibatches in Lua tables with <tt>table.insert(...)</tt></font>\n",
    "\n",
    "To complete the <tt>define_training.lua</tt> file you need to finish the implementation of the <tt>local feval = function(x)</tt> function.\n",
    "The <tt>feval</tt> function performs a forward and backward pass to backpropagate the gradients computed from the minibatches.\n",
    "\n",
    "\n",
    "### <font color='green'>Question A.6</font>\n",
    "\n",
    "<font color='green'>\n",
    "In the <tt>define_training.lua</tt> file finish the implementation of the feval function, see <tt>TODO2</tt>.\n",
    "Given the input minibatches you first need to perform a forward pass and get the output of the network and the respective loss.\n",
    "The you perform a step of backpropagation to compute the gradient estimates as the sum of the gradients computed per sample in the minibatch.\n",
    "Optionally, you might want to update the confusion matrix so that you have an overview of the training progress.\n",
    "In the end do not forget to divide the minibatch error and the minibatch gradient by the number of samples in the minibatch.</font>\n",
    "\n",
    "You do not need to add code in other lines other than the ones that are obviously incomplete, followed by a comment <tt>--  COMMENT</tt>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3 Define MLP model\n",
    "\n",
    "Once we have loaded and preprocessed the data, we need to define the architecture of our model.\n",
    "Defining the architecture of a neural network in Torch is pretty straightforward.\n",
    "First, we need to define the container of the network, which can be either one of the following:\n",
    "```lua\n",
    "require 'nn'\n",
    "...\n",
    "nn.Sequential()\n",
    "nn.Parallel()\n",
    "nn.Concat()\n",
    "```\n",
    "\n",
    "The <tt>nn.Sequential()</tt> connects modules in a feedforward manner.\n",
    "The <tt>nn.Parallel()</tt> feeds parts of data to different parts of the network resembling a parallel structure.\n",
    "The <tt>nn.Concat()</tt> concatenates the outputs of modules from a parallel container.\n",
    "As the MLP is a feedforward neural network, we opt for </tt>nn.Sequential()</tt>.\n",
    "Note that after calling the <tt>nn.Sequential()</tt> container, we call\n",
    "\n",
    "```lua\n",
    "nn.Reshape(D)\n",
    "```\n",
    "where $D$ is the feature dimensionality.\n",
    "This function flattens the input, so that there are no issues with the geometry of the features.\n",
    "\n",
    "Next, we need to define the rest of the modules of the architecture <b>and</b> the loss function.\n",
    "Open the <tt>define_model.lua</tt> file and fill in the code in the <tt>TODO</tt> sections according to the following questions.\n",
    "\n",
    "### <font color='green'>Question A.7 [+5 points]</font>\n",
    "\n",
    "<font color='green'>\n",
    "Define a network composed of the modules: a linear module $\\rightarrow$ a $tanh(\\cdot)$ module $\\rightarrow$ a linear module $\\rightarrow$ a log-softmax module.\n",
    "Print the model.\n",
    "What kind of output does the log-softmax module return?\n",
    "In the default code we are using the negative log-likelihood criterion.\n",
    "Write down the loss function this criterion minimizes.</font>\n",
    "\n",
    "Having defined your training model, you are almost ready to run your first experiment.\n",
    "Before you do that, you first also need to define your testing script, where you test the accuracy of your model during training, or after the training for the final evaluations.\n",
    "For your convenience we have implemented thi script for you.\n",
    "The script is implemented in the function <tt>define_testing.lua</tt>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4 Tune the network and the hyper-parameters \n",
    "### <font color='green'> [Questions A.8-A.12: +30 points] </font>\n",
    "\n",
    "If you have completed correctly all the parts so far, you should be able to run a full A to Z experiment by typing</font>\n",
    "\n",
    "```lua\n",
    "dofile 'assignment2.lua'\n",
    "```\n",
    "\n",
    "Furthermore, although the default settings might work ok, there might be different choices for an architecture, loss function and hyperparameters.\n",
    "\n",
    "### <font color='green'>Question A.8</font>\n",
    "\n",
    "<font color='green'>\n",
    "Run your MLP using the default, suggested parameters.\n",
    "Print the accuracy results per class and overall.\n",
    "Plot and report the training and testing loss, as well as the training and testing accuracy during the different training epochs.</font>\n",
    "\n",
    "### <font color='green'>Question A.9</font>\n",
    "\n",
    "<font color='green'>\n",
    "Experiment with different architectures.\n",
    "In the original model we had a linear module $\\rightarrow$ a $tanh(\\cdot)$ module $\\rightarrow$ a linear module $\\rightarrow$ a log-softmax module.\n",
    "Now try increasing the depth, namely define a new model composed of:\n",
    "<ul>\n",
    "<li>a linear module</li>\n",
    "<li>a $tanh(\\cdot)$ module</li>\n",
    "<li>a linear module</li>\n",
    "<li>a $tanh(\\cdot)$ module</li>\n",
    "<li>a log-softmax module.</li>\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "Then, try replacing in the original network the $tanh(\\cdot)$ with sigmoid modules, then with ReLU modules.\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "Try a combination of the two, namely replace the modules in the deeper network with the activation functions that worked best.\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "In this last network try increasing the number of neurons per module.\n",
    "What are your observations?\n",
    "</font>\n",
    "\n",
    "### <font color='green'>Question A.10</font>\n",
    "\n",
    "<font color='green'>\n",
    "Experiment with different loss functions.\n",
    "Try using the negative log likelihood instead of the margin loss.\n",
    "What are your observations?\n",
    "</font>\n",
    "\n",
    "### <font color='green'>Question A.11</font>\n",
    "\n",
    "<font color='green'>\n",
    "Experiment with different optimization methods.\n",
    "Try using the ASGD optimization and ADAM.\n",
    "What are your observations?\n",
    "</font>\n",
    "\n",
    "### <font color='green'>Question A.12</font>\n",
    "\n",
    "<font color='green'>\n",
    "Tune the hyperparameters so that you have the best possible test accuracy.\n",
    "For one, include regularization by increasing the weight decay hyperparameter.\n",
    "What are your observations?\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 Backpropagation and implementing your own module\n",
    "\n",
    "Until now you have made use of the already existing modules implemented in Torch.\n",
    "However, the real power of deep learning and neural networks is that one can easily implement new modules and integrate them.\n",
    "\n",
    "To implement a new module that is compatible with backpropagation, you must first define the forward pass of the module, $a=h(x;\\theta)$, where $a$ are the activations and $h(\\cdot)$ the module function.\n",
    "Implementing the forward pass is relatively straightforward.\n",
    "Then, you must define the backward pass used by backpropagation to compute the gradients of the module with respect to the input $x$ and the module parameters $\\theta$, if the module has trainable parameters.\n",
    "Implementing the backward pass is more complicated.\n",
    "\n",
    "### <font color='green'>Question B.1 [+10 points]</font>\n",
    "\n",
    "<font color='green'>\n",
    "Derive and write the backpropagation equations for a softmax module.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1 Implement a non-parametric module\n",
    "\n",
    "In Torch is quite easy to implement a new module.\n",
    "One simply needs to open a new file, and overload the following functions.\n",
    "```lua\n",
    "require 'nn'\n",
    "\n",
    "local MyModule, Parent = torch.class(...)\n",
    "-- Let's assume your new module is of the form a=h(x)\n",
    "\n",
    "function MyModule:__init(...)\n",
    "   ...\n",
    "end\n",
    "\n",
    "function MyModule:updateOutput(...)\n",
    "   ...\n",
    "end\n",
    "\n",
    "function MyModule:updateGradInput(...)\n",
    "   ...\n",
    "end\n",
    "\n",
    "function MyModule:accGradParameters(...)\n",
    "   ...\n",
    "end\n",
    "```\n",
    "\n",
    "It is not necessary to overload <tt>accGradParameters</tt>, you only need to so if your module is parametric, that is if it contains trainable parameters.\n",
    "\n",
    "### <font color='green'>Question B.2 [+10 points]</font>\n",
    "\n",
    "<font color='green'>\n",
    "Implement a rectified quadratic unit (ReQU), $f(n) = \\text{ReQU}(x) = \\begin{cases} x^2 &\\mbox{if } x \\geq 0 \\\\ 0 & \\mbox{if } x < 0 \\end{cases}$.\n",
    "The starter code can be found in <tt>MyModules/MyRequ.lua</tt>\n",
    "The ReQU is not actually a module that is used in neural networks, however, it serves as a good example for implementing a new module.\n",
    "You need to overload the <tt>__init(...)</tt>, <tt>updateOutput(...)</tt> and <ttt>updateGradInput(...)</tt>.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2 Gradient checks and Jacobians\n",
    "\n",
    "One of the most dangerous parts in designing your own modules is implementing the gradient formulations.\n",
    "We can eliminate the doubts if we check our gradients with the numerical gradients, as discussed in the lectures and the lecture notes.\n",
    "\n",
    "### <font color='green'>Question B.3 [+5 points]</font>\n",
    "\n",
    "<font color='green'>\n",
    "Open the <tt>check_module_gradients.lua</tt> and fill in the missing code (1-line).\n",
    "Check the Jacobians of your ReQU module and make sure that the implementation of your module is correct.\n",
    "Report your error differences, which must be below the provided error threshold to be correct.\n",
    "</font>\n",
    "\n",
    "\n",
    "### <font color='green'>Question B.4 [+10 points]</font>\n",
    "\n",
    "<font color='green'>\n",
    "By now you should have verified that the implementation of your fancy new ReQU module is correct, namely the gradients are computed correctly.\n",
    "The rest of the modules are provided by Torch, so we can assume they have already been checked.\n",
    "However, it is good practice to generally check our full network gradients, just to make sure.\n",
    "Open the <tt>check_model_gradients.lua</tt> and fill in the missing code.\n",
    "</font>\n",
    "\n",
    "\n",
    "### <font color='green'>Question B.5 [+5 points]</font>\n",
    "\n",
    "<font color='green'>\n",
    "Now you know the full backpropagation of your neural network is correct.\n",
    "Start from your default network of the previous subsection.\n",
    "Replace the $tanh(\\cdot)$ module with a ReQU module.\n",
    "Run your new network.\n",
    "Plot again the accuracies and the losses during the training\n",
    "What do you observe? \n",
    "Does the new module improve the results?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='purple'>Section 4 For enthusiastic students [Optional, +30 points]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>Section 4.1 Implement backpropagation [+15 points]</font>\n",
    "\n",
    "Reimplement the backpropagation for the original network composed of.\n",
    "Check your gradients.\n",
    "Are they correct?\n",
    "Run the same experiment as for Question A.8.\n",
    "Do you get similar results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>Section 4.2 Implement your own modules [+15 points]</font>\n",
    "Implement a parametric module, whose activation is according $a(x)=sin(Wx)$.\n",
    "Note that since this is a parametric module, you have to also overload the function <tt>accGradParameters</tt>.\n",
    "Check the Jacobians and gradients and see if these module makes your network more accurate.\n",
    "In your report include also the gradient formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
